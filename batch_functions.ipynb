{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "765bfb09-fc33-463a-994a-07fe040925a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2281646b-60ab-4c5b-b88e-6528a4346898",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Packages Importing\n",
    "import datetime\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import pyspark.pandas as ps\n",
    "import time\n",
    "\n",
    "from math import ceil\n",
    "\n",
    "# ======== FUNÇÃO DE EXTRAÇÃO DOS DADOS DA API ========\n",
    "API_KEY = m_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f70d615c-c84b-4c23-9837-fa8b8b134c52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# =========================================\n",
    "\n",
    "# ============ BATCH FUNCTIONS ============\n",
    "\n",
    "# ========================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bef0b7b-d4a8-43f9-b97a-8c61a8c2b2d5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ======== FUNÇÃO DE EXTRAÇÃO DOS DADOS DA API ========\n",
    "API_KEY = m_api_key\n",
    "\n",
    "# ======== FUNÇÃO DE EXTRAÇÃO DOS DADOS DA API ========\n",
    "def p(string):\n",
    "    print('\\n', string, sep='')\n",
    "\n",
    "def gerar_query_string():\n",
    "    # ------------ Filtro de palavras -------------- #\n",
    "    # Lista de palavras chaves\n",
    "    key_words = [\n",
    "        'genomics', 'genetic sequencing', 'dna', 'genes',\n",
    "        'genomic analysis', 'genetic variations', 'targeted therapy',\n",
    "        'customized treatment', 'genetic markers', 'molecular diagnosis',\n",
    "        'pharmacogenomics', 'personalized medicine'\n",
    "        ]\n",
    "    \n",
    "    key_words_3 = [\n",
    "        'genomics', 'genetic', 'dna',\n",
    "        ]\n",
    "\n",
    "    for idx, word in enumerate(key_words_3):\n",
    "        if ' ' in word:\n",
    "            word = word.replace(' ', ' AND ')\n",
    "            key_words_3[idx] = '(' + word + ')' \n",
    "    \n",
    "    return ' OR '.join(key_words_3)\n",
    "\n",
    "\n",
    "# Função 1\n",
    "def get_raw_news_files(\n",
    "        query='genomic', \n",
    "        start_date='2023-09-26', \n",
    "        end_date='2023-09-26', \n",
    "        start_time='00:00:00', \n",
    "        end_time='01:00:00') -> None:\n",
    "    \"\"\" \n",
    "    Faz a requisição à API e persiste em um arquivo JSON dentro do DBFS \n",
    "        As diversas páginas da pesquisa são salvas em arquivos JSON separados\n",
    "        no diretório [FileStore/proj_4_genomics_sa/raw/queue]. \n",
    "    \"\"\"\n",
    "\n",
    "    # Variáveis da consulta\n",
    "    page_n = 1\n",
    "    page_size = 100\n",
    "    total_pages = 1\n",
    "\n",
    "    # Formata as datas e horários\n",
    "    FROM = f'&from={start_date}T{start_time}'\n",
    "    TO = f'&to={end_date}T{end_time}'\n",
    "\n",
    "    # Configurações da API da consulta\n",
    "    QUERY = f'q={query}'\n",
    "    PAGE = f'&page={page_n}'\n",
    "    LANGUAGE = '&language=en'\n",
    "    SORTBY = '&sortBy=relevancy'\n",
    "\n",
    "    while True:\n",
    "        if (page_n > total_pages) or (page_n >= 5):\n",
    "            break\n",
    "        \n",
    "        # ---------- Conexão com a API ------------ #\n",
    "        # Monta a URL da API\n",
    "        api_url = f'https://newsapi.org/v2/everything?{QUERY}{FROM}{TO}{SORTBY}{LANGUAGE}{PAGE}&apiKey={API_KEY}'\n",
    "        # print(f'URI: {api_url}')\n",
    "        \n",
    "        # Faz a requisição à API\n",
    "        response = requests.get(api_url)\n",
    "        \n",
    "        # Persistência do conteúdo\n",
    "        if response.status_code == 200:\n",
    "            # Arquivo JSON\n",
    "            now = datetime.datetime.now()\n",
    "            json_folder_path = '/tmp/proj'\n",
    "            json_file_path = f'/newsapi-get_{now.strftime(\"%Y-%m-%dT%H_%M_%S\")}P{page_n}.json'\n",
    "            json_uri = json_folder_path + json_file_path        \n",
    "\n",
    "            # Verifica se diretório já existe\n",
    "            if not os.path.exists(json_folder_path):\n",
    "                os.makedirs(json_folder_path)\n",
    "            \n",
    "            # Armazena o json em bloco (driver node)\n",
    "            data_json = response.json()\n",
    "            with open(json_uri, 'w') as json_file:\n",
    "                json.dump(data_json, json_file)\n",
    "            \n",
    "            # Move para armazenamento em objeto (DBFS)\n",
    "            json_folder_dbfs = 'dbfs:/FileStore/proj_4_genomics_sa/raw/queue'\n",
    "            dbutils.fs.mv(f'file:{json_uri}', f'{json_folder_dbfs}{json_file_path}')\n",
    "\n",
    "            # Paginação\n",
    "            resultados = data_json['totalResults']\n",
    "            total_pages = ceil( resultados / page_size )\n",
    "            page_n += 1\n",
    "            PAGE = f'&page={page_n}'\n",
    "\n",
    "            # log\n",
    "            if page_n-1 == 1:\n",
    "                print(f'A requisição possui {resultados} notícias em {total_pages} páginas.')\n",
    "\n",
    "            print(f'Arquivo JSON salvo em DBFS: {json_folder_dbfs}{json_file_path}')\n",
    "\n",
    "        else:\n",
    "            # Falha na persistência do arquivo\n",
    "            print(f'Falha na requisição! Status code: {response.status_code}')\n",
    "            resultados = 0\n",
    "            total_pages = 0\n",
    "            break\n",
    "\n",
    "    print(f'Extração por REST API concluída: {resultados} resultados coletados em {page_n} páginas.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49d588e4-e3c8-4fbb-a1bc-86575159e806",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------- sub-ETL: Fluxo de JSON separados para PARQUET unificado ----------\n",
    "\n",
    "def func_convert_to_df(json_obj: dict) -> ps.DataFrame:\n",
    "    \"\"\" Função de tabulação dos dados do JSON da NewsAPI em um PyPandas \"\"\"\n",
    "    # Coleção de registros\n",
    "    tabular_data = []\n",
    "\n",
    "    for row in json_obj['articles']: \n",
    "        # ----------- Organização dos dados ------------ #\n",
    "        dict_data = {}\n",
    "\n",
    "        dict_data['id']      = row.get('source', '').get('id', '')\n",
    "        dict_data['name']    = row.get('source', '').get('name', '')\n",
    "        dict_data['author']  = row.get('author', '')\n",
    "        dict_data['title']   = row.get('title', '')\n",
    "        dict_data['url']     = row.get('url', '')\n",
    "        dict_data['content'] = row.get('content', '')\n",
    "        dict_data['description'] = row.get('description', '')\n",
    "        dict_data['publishedAt'] = row.get('publishedAt', '')\n",
    "\n",
    "        # Adicição à coleção de registros\n",
    "        tabular_data.append(dict_data)\n",
    "    return ps.DataFrame(tabular_data)\n",
    "\n",
    "\n",
    "# Função 2\n",
    "def union_raw_news_pages() -> dict:\n",
    "    \"\"\" Função converte páginas JSON em um único arquivo estruturado Parquet. \n",
    "    \n",
    "    RETORNO\n",
    "    -------\n",
    "        Dict[path: str]\n",
    "        Retorna um dicionário com informação do arquivo Parquet gerado.\n",
    "    \"\"\"\n",
    "\n",
    "    arquivos = dbutils.fs.ls('file:/tmp/proj_raw')\n",
    "    timestamp = arquivos[0].name.split('_', maxsplit=1)[-1].split('P')[0]\n",
    "\n",
    "    # DataFrame unido\n",
    "    df = ps.DataFrame()\n",
    "\n",
    "    n = 0\n",
    "    for arquivo in arquivos:\n",
    "        # Path DBFS para padrão File (Drive Node)\n",
    "        json_path = arquivo.path.replace('file:', '')\n",
    "\n",
    "        # Extrai dados de JSON\n",
    "        with open(json_path, 'r') as json_file:\n",
    "            json_data = json.load(json_file)\n",
    "\n",
    "        # Transformação de JSON para DataFrame\n",
    "        df_temp = func_convert_to_df(json_data)\n",
    "\n",
    "        # Concatena df do arquivo com o df unificado\n",
    "        df = ps.concat([df, df_temp], ignore_index=True)\n",
    "        df['extract_date'] = timestamp\n",
    "\n",
    "        # Log\n",
    "        n += 1\n",
    "        print(f'{n} de {len(arquivos)} arquivos processados')\n",
    "    \n",
    "    # Persiste dados em parquet\n",
    "    file_path = f'dbfs:/FileStore/proj_4_genomics_sa/validated/api_responses/noticias_{timestamp}.parquet'\n",
    "\n",
    "    # df.reset_index().to_parquet(file_path)\n",
    "    df.to_parquet(file_path)\n",
    "\n",
    "    print('Páginas juntadas com sucesso em [./validated/api_responses/]')\n",
    "    return {'path': file_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8a85534-9ca1-4927-895f-bbc86701db1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Função 3\n",
    "def load_into_unified_file(news_filepath: str) -> None:\n",
    "    \"\"\"\n",
    "    Função carrega arquivo novo de notícias e uni com o arquivo unificado.\n",
    "\n",
    "    ARGUMENTOS\n",
    "    ----------\n",
    "    news_filepath : str\n",
    "        Caminho do arquivo das novas notícias.\n",
    "    \"\"\"\n",
    "\n",
    "    # Leitura do arquivo com as novos notícias\n",
    "    df_news = ps.read_parquet(news_filepath)\n",
    "    print('Tamanho do arquivo novo:', df_news.shape)\n",
    "\n",
    "    # Caminho do arquivo unificado\n",
    "    noticias_filepath = 'dbfs:/FileStore/proj_4_genomics_sa/validated/noticias.parquet'\n",
    "\n",
    "    try:\n",
    "        # Leitura da base unificada de notícias\n",
    "        arquivo = dbutils.fs.ls(noticias_filepath)\n",
    "        # df_unified = spark.read.parquet(noticias_filepath)\n",
    "        df_unified = ps.read_parquet(noticias_filepath)\n",
    "        print('Tamanho arquivo unificado:', df_unified.shape)\n",
    "\n",
    "        # Concatenação dos dados\n",
    "        df_unified = ps.concat([df_news, df_unified])\n",
    "\n",
    "        # Descatas linhas duplicadas com base na url mantendo o artigo mais recente\n",
    "        df_unified = df_unified.sort_values(by=['extract_date'])\n",
    "\n",
    "        key = 'url'\n",
    "\n",
    "        df_unified = df_unified.drop_duplicates(subset=key, keep='last') \n",
    "\n",
    "        print('Tamanho novo arquivo unificado:', df_unified.shape)\n",
    "        \n",
    "        # Persiste no mesmo arquivo\n",
    "        df_unified.to_parquet(noticias_filepath)\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        # Caso o arquivo unificado não exista ainda\n",
    "        if 'java.io.FileNotFoundException' in str(e):\n",
    "            print(\"Arquivo não encontrado, primeiro processamento\")\n",
    "            df_news.to_parquet(noticias_filepath)\n",
    "            print('Tamanho novo arquivo unificado:', df_news.shape)\n",
    "        else:\n",
    "            raise e\n",
    "    finally:\n",
    "        print(\"Resultado carregado com sucesso\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "117f0278-0c63-4c9f-9a94-5641f492fc8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def noticias_por_ano_mes_dia(df):\n",
    "    # Convert a coluna para datetime\n",
    "    df['published_data'] = df['publishedAt'].apply(lambda x: x.split('T')[0])\n",
    "    df['published_data'] = ps.to_datetime(df['published_data'])\n",
    "\n",
    "    # Criar colunas para ano, mês e dia\n",
    "    df['year'] = df['published_data'].dt.year\n",
    "    df['month'] = df['published_data'].dt.month\n",
    "    df['day'] = df['published_data'].dt.day\n",
    "    \n",
    "    # Consolidação 1: Quantidade de notícias por ano, mês e dia de publicação\n",
    "    noticias_por_dia = df.groupby(['year', 'month', 'day']).size().reset_index(name='news_qtd')\n",
    "\n",
    "    file_path = f'dbfs:/FileStore/proj_4_genomics_sa/curated/'\n",
    "\n",
    "    noticias_por_dia.to_parquet(file_path + 'noticias_por_dia.parquet')\n",
    "\n",
    "    #return noticias_por_ano, noticias_por_mes, noticias_por_dia\n",
    "\n",
    "def noticias_por_fonte_autor(df):\n",
    "    # Consolidação 2: Quantidade de notícias por fonte e autor\n",
    "    noticias_por_fonte = df.groupby('name').size().reset_index(name='news_qtd')\n",
    "    noticias_por_autor = df.groupby('author').size().reset_index(name='news_qtd')\n",
    "\n",
    "    file_path = f'dbfs:/FileStore/proj_4_genomics_sa/curated/'\n",
    "\n",
    "    noticias_por_fonte.to_parquet(file_path + 'noticias_por_fonte.parquet')\n",
    "    noticias_por_autor.to_parquet(file_path + 'noticias_por_autor.parquet')\n",
    "\n",
    "    #return noticias_por_fonte, noticias_por_autor\n",
    "\n",
    "def contagem_palavras_chaves(df):\n",
    "    # Consolidação 3: Quantidade de aparições de 3 palavras chaves por ano, mês e dia de publicação\n",
    "    keywords = ['genomics', 'dna', 'genes']\n",
    "\n",
    "    df['conteudo_lowercase'] = df['content'].str.lower()\n",
    "    \n",
    "    for word in keywords:\n",
    "        df[word] = df['conteudo_lowercase'].str.count(word)\n",
    "    \n",
    "    df = df.drop(columns=['conteudo_lowercase'])\n",
    "    \n",
    "    aparicoes_por_dia = df.groupby(['year', 'month', 'day'])[keywords].sum()\n",
    "\n",
    "    file_path = f'dbfs:/FileStore/proj_4_genomics_sa/curated/'\n",
    "\n",
    "    aparicoes_por_dia.to_parquet(file_path + 'aparicoes_por_dia.parquet')\n",
    "    \n",
    "    #return aparicoes_por_ano, aparicoes_por_mes, aparicoes_por_dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11b587d2-12f3-4f8e-8aee-6b24cbe49518",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ===================================\n",
    "\n",
    "# ============ BATCH ETL ============\n",
    "\n",
    "# ==================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "245f8e50-d134-4843-84e2-9cd661fb8e9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def etl_news( end_datetime: datetime.datetime ) -> None:\n",
    "    \"\"\" Função main do ETL de novas notícias sobre o campo da Genômica. \n",
    "    \n",
    "    ARGUMENTS\n",
    "    ---------\n",
    "        end_datetime : datetime.datetime\n",
    "            Data e horário do fim da execução da extração. O horário de início será \n",
    "            calculado para 1 hora antes.\n",
    "\n",
    "    CALL\n",
    "    ----\n",
    "    end_time = '2023-10-03T10:55:39'\n",
    "    etl_news( datetime.datetime.fromisoformat(end_time) )\n",
    "    \"\"\"\n",
    "    \n",
    "    p('======== REQUISIÇÕES REST =========')\n",
    "    # Query de busca\n",
    "    # query = 'genomic'\n",
    "    query = gerar_query_string()\n",
    "\n",
    "    # ------------- Filtro de horário -------------- #\n",
    "    end_date = end_datetime.strftime('%Y-%m-%d')\n",
    "    end_time = end_datetime.strftime('%H-%M-%S')\n",
    "    \n",
    "    start_datetime = end_datetime - datetime.timedelta(hours=1)\n",
    "    start_date = start_datetime.strftime('%Y-%m-%d')\n",
    "    start_time = start_datetime.strftime('%H-%M-%S')\n",
    "    \n",
    "\n",
    "    # Extrai dados da API e consolida no DBFS\n",
    "    get_raw_news_files(\n",
    "        query=query, \n",
    "        start_date=start_date,\n",
    "        end_date=end_date,\n",
    "        start_time=start_time,\n",
    "        end_time=end_time)\n",
    "    \n",
    "    p('======== UNIÃO DAS PÁGINAS =========')\n",
    "    # Adaptação para o Community Edition :: copia arquivo para uma pasta do Driver Node\n",
    "    dbutils.fs.cp('dbfs:/FileStore/proj_4_genomics_sa/raw/queue', 'file:/tmp/proj_raw', True)\n",
    "\n",
    "    # Transforma JSON da consulta em Parquet\n",
    "    novo_arquivo = union_raw_news_pages()\n",
    "\n",
    "    # Movimenta arquivos lidos\n",
    "    dbutils.fs.mv('dbfs:/FileStore/proj_4_genomics_sa/raw/queue', 'dbfs:/FileStore/proj_4_genomics_sa/raw/readed', True)\n",
    "    print('Arquivo movidos para [readed]')\n",
    "\n",
    "    p('======== CARGA DE TODAS NOTÍCIAS EM ARQUIVO UNIFICADO =========')\n",
    "    # Carrega em arquivo unificado\n",
    "    load_into_unified_file(novo_arquivo['path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cd63aa7-143c-40fc-91ac-19bf7e9022d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def etl_consolidacao(_):\n",
    "    # Leitura da base unificada de notícias\n",
    "    print('ETL de consolidação sendo inicializado...')\n",
    "    noticias_filepath = 'dbfs:/FileStore/proj_4_genomics_sa/validated/noticias.parquet'\n",
    "    df = ps.read_parquet(noticias_filepath)\n",
    "\n",
    "    # 4.1 - Quantidade de notícias por ano, mês e dia de publicação\n",
    "    print('Consolidando as notícias por dia, mês e ano....', end='')\n",
    "    noticias_por_ano_mes_dia(df)\n",
    "    print('concluído.')\n",
    "\n",
    "    # 4.2 - Quantidade de notícias por fonte e autor\n",
    "    print('Consolidando as notícias por fonte e autor....', end='')\n",
    "    noticias_por_fonte_autor(df)\n",
    "    print('concluído.')\n",
    "\n",
    "    # 4.3 - Quantidade de aparições de 3 palavras chaves por ano, mês e dia de publicação\n",
    "    print('Consolidando as notícias por 3 palavras e dia, mês e ano....', end='')\n",
    "    contagem_palavras_chaves(df)\n",
    "    print('concluído.')\n",
    "\n",
    "    print('Conclusão de consolidação! Arquivos salvos em [./curated]\\n')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "batch_functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
